{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elegant-monkey",
   "metadata": {},
   "source": [
    "## Portfolio Construction\n",
    "\n",
    "\n",
    "Instead of trying to limit the exposure each day I am just looking to ensure that we do not make too many trades based on one factor. This involves initially setting up our portfolio to match an exposure level, and from there and future days simply maintaining roughly the same number of trades towards each factor. So for instance if we want equal factor exposure, we limit the factor exposure initially to 12.5%. \n",
    "\n",
    "\n",
    "One thing to note here is that this corresponds to less than 5 trades, at a stock price of 250,000 and so we will never have more than 5 open trades towards one factor at any point in time. Even if the net exposure remained low (say three buys two short sell) but because we do not have infinite money, if we maximized our net exposure by taking more trades (up to reaching the 12.5% threshold) we'd exceed our 10,000,000 limit.\n",
    "\n",
    "For this reason, though it may mean practically missing out on trades, for now I set a \"hard cap\" of 5 trades for each factor which will not be exceeded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-azerbaijan",
   "metadata": {},
   "source": [
    "## Imports & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forbidden-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General formatting. I use japan as it is relatively small. Only important point here is that datetime needs\n",
    "#reformated for easy filtering. This is done repeatedly throughout the document as it appears to reset\n",
    "#I am initially only working with 2017 again just for practical reasons. \n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "japan_trades = pd.read_csv('Japan Trades.csv')\n",
    "#price_data = pd.read_pickle('price_df.pkl')\n",
    "#fx_data = pd.read_excel('FX.xlsx',sheet_name='data', skiprows=4, index_col=0)\n",
    "#mapping = pd.read_csv('mapping.csv')\n",
    "\n",
    "japan_trades['Date'] = pd.to_datetime(japan_trades['Date'])\n",
    "#japan_2017 = japan_trades[japan_trades['Date'].dt.year == 2017]\n",
    "japan_2017 = japan_trades.drop(columns=['returns', 'company_name'])\n",
    "Days = japan_2017.Date.unique()\n",
    "\n",
    "data = japan_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-millennium",
   "metadata": {},
   "source": [
    "## Variable Set Up\n",
    "\n",
    "\n",
    "\n",
    "Note: there are lots of NAs that remain in. These are related to the buy covers, and sells, which I deal with by setting the end date to be the same as the start date. Otherwise filtering for NA's removes this data. They appear in the \"Exit Date\" and \"return\" columns but I never explicitly use these. The reason for this is that I have found on one day, we may have multiple trades associated with the same fsym_id being closed. This is an issue, and so I simply do not deal with the \"buy cover\" or \"sell\" options for \"Side\", instead just using the \"Exit Date\" as I assume every trade must be exercised (as I believe these are not options...?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crude-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val = 10000000 \n",
    "trade_val_init = 250000\n",
    "\n",
    "#This is effectively a measure of the exposure. For the given numbers we get a value of 1/40, and so if we want\n",
    "#our maximum factor exposure to be 12% for each factor this means we cannot have a \" 5 trade swing\" in favour of \n",
    "#buy or sell. For example if we had 14 buy trades and 10 sell shorts this is fine, but if it was 15 and 10 we \n",
    "#would be over exposed to the given factor. \n",
    "\n",
    "stock_weighting = np.round(trade_val_init/total_val,3)\n",
    "\n",
    "#unique factors and sectors. This is importantly for all days, for each day we may not have a factor / sector present\n",
    "#and so these need to be re-defined daily. \n",
    "\n",
    "factors = list(data['Factor'].unique())\n",
    "sectors = list(data['Sector'].unique())\n",
    "\n",
    "#inbuilt maximum net and sector exposure. Can be changed. \n",
    "maximum_net_exposure = np.round(1/len(factors),2)\n",
    "maximum_sector_exposure = np.round(100/len(sectors),0)\n",
    "\n",
    "#this is set for now, but note that as prices change in the future we may be able to change this. \n",
    "maximum_trades = np.round(total_val/trade_val_init,3)\n",
    "maximum_factor_trades = int(maximum_trades/len(factors))\n",
    "\n",
    "#This will contain the portfolio for each given day \n",
    "portfolios = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-international",
   "metadata": {},
   "source": [
    "## Valid Exposure Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occasional-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make code more efficient, this valid_exposure function is called first to see if our input is already valid.\n",
    "#It is checking that the length (i.e. number of trades) for a given factor does not exceed the allocation\n",
    "#for instance the ML models may have given us 10 datapoints for a given factor / day in the Japan Trades.csv file\n",
    "#and this ensures we do not exceed that. It also ensures we do not allocate too much of our portfolio to one single factor. \n",
    "\n",
    "def valid_exposure(data,maximum_net_exposure,max_trades):\n",
    "    exposure = 0\n",
    "    total_trades = 0 \n",
    "    \n",
    "    for i in [\"Sell Short\",\"Buy\"]:\n",
    "        exposure_by_side = data[data.Side == i]\n",
    "        total_trades += len(exposure_by_side)\n",
    "        \n",
    "        if i == \"Sell Short\":\n",
    "            exposure += stock_weighting*len(exposure_by_side)\n",
    "        else:\n",
    "            exposure -= stock_weighting*len(exposure_by_side)\n",
    "    \n",
    "    if np.abs(exposure) <= maximum_net_exposure and total_trades <= max_trades:\n",
    "        print(\"hes valid\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-gather",
   "metadata": {},
   "source": [
    "## Return Valid Initial Portfolio Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demanding-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data                 : should be a dataframe, pre sorted by both date and factor.\n",
    "#maximum_net_exposure : the maximum exposure to a given factor. Initially I take them all as 12%\n",
    "#max_trades           : the maximum number of trades we can make for each factor. \n",
    "\n",
    "def get_valid(data,maximum_net_exposure,max_trades):\n",
    "    data =data.drop_duplicates()\n",
    "    #calls simpler function for efficiency. \n",
    "    if valid_exposure(data,maximum_net_exposure,max_trades) == True:\n",
    "        return data\n",
    "    else: \n",
    "        \n",
    "        #sets up our blank portfolio representing the day,factor pair. \n",
    "        portfolio = data.head(1)\n",
    "        \n",
    "        #extracts positions that are buy or sell short. \n",
    "        buys = data[data.Side == \"Buy\"]\n",
    "        sell_shorts = data[data.Side == \"Sell Short\"]\n",
    "        \n",
    "        #this difference in length can be seen as the \"over-exposure\"? For instance if we had 80 buys and 70 short sells\n",
    "        #we would have 10 difference, and after multiplying by the (initially equal) weighting of each of these\n",
    "        #we would arrive at 10*0.025 = 0.25 = over exposed! \n",
    "        \n",
    "        difference = np.abs(len(buys) - len(sell_shorts))\n",
    "\n",
    "        #this sets up our dataframe first by filling with pairs of buy and sell shorts to ensure net_exposure\n",
    "        #remains un-changed. Entries here are going to be filtered down later in the code.\n",
    "        for i in range(0,min(len(buys),len(sell_shorts))):\n",
    "            portfolio = portfolio.append(buys.iloc[[i]])\n",
    "            portfolio = portfolio.append(sell_shorts.iloc[[i]])\n",
    "            \n",
    "        portfolio = portfolio.iloc[1: , :]\n",
    "        trade_limit = min(difference,max_trades)\n",
    "        \n",
    "        #adds our new trades to the portfolio. If we have more buys we add buys, etc. \n",
    "        if len(buys) > len(sell_shorts):\n",
    "            for k in range (0,trade_limit):\n",
    "                    portfolio = portfolio.append(buys.iloc[[i+k]])\n",
    "        elif len(sell_shorts) < len(buys):\n",
    "             for k in range (0,trade_limit):\n",
    "                    portfolio = portfolio.append(sell_shorts.iloc[[i+k]])\n",
    "                    \n",
    "        #this bit is just ensuring we do not make too many trades. It is checking the length of our constructed portfolio\n",
    "        #If too long, it removes a pair. This will ensure our portfolio remains within 1 unit of the maximum allocated\n",
    "        #trades. \n",
    "        if len(portfolio) > max_trades:\n",
    "            difference = len(portfolio) - 1 - max_trades\n",
    "            if difference % 2 == 0:\n",
    "                sample = int(difference/2)\n",
    "                for i in [\"Sell Short\",\"Buy\"]:\n",
    "                    if i == \"Sell Short\":\n",
    "                        portfolio = portfolio.drop(portfolio[portfolio['Side'] == i].sample(n= sample).index)\n",
    "                    else:\n",
    "                        portfolio = portfolio.drop(portfolio[portfolio['Side'] == i].sample(n= sample + 1).index)\n",
    "            else:\n",
    "                sample = int((-1 + difference)/2 + 1)     \n",
    "                for i in [\"Sell Short\",\"Buy\"]:\n",
    "                    remove = portfolio[portfolio['Side'] == i].sample(n= sample).index\n",
    "\n",
    "                    portfolio = portfolio.drop(remove)\n",
    "        #removes the first row that was needed for set-up.\n",
    "        \n",
    "        portfolio = portfolio.reset_index(drop = True)\n",
    "        return portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-harmony",
   "metadata": {},
   "source": [
    "## Initial Portfolio Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plastic-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>Side</th>\n",
       "      <th>Date</th>\n",
       "      <th>Exit Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Factor</th>\n",
       "      <th>Actual Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>C7P0T8-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Momentum</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>HLXGRS-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Momentum</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158</td>\n",
       "      <td>HTLNY7-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Momentum</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162</td>\n",
       "      <td>GW6XYJ-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Momentum</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175</td>\n",
       "      <td>D2WLKV-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Momentum</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54</td>\n",
       "      <td>QH66VR-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Consumer_Discretionary</td>\n",
       "      <td>Behavioural</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>D66200-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Consumer_Discretionary</td>\n",
       "      <td>Behavioural</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>RCHFSX-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Consumer_Discretionary</td>\n",
       "      <td>Behavioural</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>92</td>\n",
       "      <td>T0LBNC-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Consumer_Discretionary</td>\n",
       "      <td>Behavioural</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>NGSSGB-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Consumer_Discretionary</td>\n",
       "      <td>Behavioural</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>228</td>\n",
       "      <td>DLTMTH-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Liq and Size</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>234</td>\n",
       "      <td>W3FKLX-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Liq and Size</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>81</td>\n",
       "      <td>JVJ65W-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Liq and Size</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>242</td>\n",
       "      <td>WFH1P7-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Liq and Size</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>104</td>\n",
       "      <td>WZKC5Z-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>Japan</td>\n",
       "      <td>All_sectors</td>\n",
       "      <td>Liq and Size</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>123</td>\n",
       "      <td>PPZMYZ-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Volatility</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>117</td>\n",
       "      <td>KNB7YM-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Information_Technology</td>\n",
       "      <td>Volatility</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>118</td>\n",
       "      <td>M5237B-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Information_Technology</td>\n",
       "      <td>Volatility</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>154</td>\n",
       "      <td>PZ2Q5Z-R</td>\n",
       "      <td>Buy</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Information_Technology</td>\n",
       "      <td>Volatility</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>120</td>\n",
       "      <td>HLS2CN-R</td>\n",
       "      <td>Sell Short</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Information_Technology</td>\n",
       "      <td>Volatility</td>\n",
       "      <td>2017-01-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   fsym_id        Side       Date   Exit Date Country  \\\n",
       "0          135  C7P0T8-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "1           49  HLXGRS-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "2          158  HTLNY7-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "3          162  GW6XYJ-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "4          175  D2WLKV-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "5           54  QH66VR-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "6           13  D66200-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "7           14  RCHFSX-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "8           92  T0LBNC-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "9           22  NGSSGB-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "10         228  DLTMTH-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "11         234  W3FKLX-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "12          81  JVJ65W-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "13         242  WFH1P7-R  Sell Short 2017-01-09  2017-01-17   Japan   \n",
       "14         104  WZKC5Z-R         Buy 2017-01-09  2017-01-17   Japan   \n",
       "15         123  PPZMYZ-R         Buy 2017-01-09  2017-01-18   Japan   \n",
       "16         117  KNB7YM-R  Sell Short 2017-01-09  2017-01-13   Japan   \n",
       "17         118  M5237B-R  Sell Short 2017-01-09  2017-01-13   Japan   \n",
       "18         154  PZ2Q5Z-R         Buy 2017-01-09  2017-01-13   Japan   \n",
       "19         120  HLS2CN-R  Sell Short 2017-01-09  2017-01-13   Japan   \n",
       "\n",
       "                    Sector        Factor Actual Date  \n",
       "0              All_sectors      Momentum  2017-01-09  \n",
       "1              All_sectors      Momentum  2017-01-09  \n",
       "2              All_sectors      Momentum  2017-01-09  \n",
       "3              All_sectors      Momentum  2017-01-09  \n",
       "4              All_sectors      Momentum  2017-01-09  \n",
       "5   Consumer_Discretionary   Behavioural  2017-01-09  \n",
       "6   Consumer_Discretionary   Behavioural  2017-01-09  \n",
       "7   Consumer_Discretionary   Behavioural  2017-01-09  \n",
       "8   Consumer_Discretionary   Behavioural  2017-01-09  \n",
       "9   Consumer_Discretionary   Behavioural  2017-01-09  \n",
       "10             All_sectors  Liq and Size  2017-01-09  \n",
       "11             All_sectors  Liq and Size  2017-01-09  \n",
       "12             All_sectors  Liq and Size  2017-01-09  \n",
       "13             All_sectors  Liq and Size  2017-01-09  \n",
       "14             All_sectors  Liq and Size  2017-01-09  \n",
       "15                  Energy    Volatility  2017-01-09  \n",
       "16  Information_Technology    Volatility  2017-01-09  \n",
       "17  Information_Technology    Volatility  2017-01-09  \n",
       "18  Information_Technology    Volatility  2017-01-09  \n",
       "19  Information_Technology    Volatility  2017-01-09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets the data for the first day\n",
    "first_day = Days[0]\n",
    "data_initial = data[data['Date'] == first_day]\n",
    "\n",
    "initial_portfolio = [] \n",
    "\n",
    "#gets the factors for which we may trade for a current day. Most days in the current data we would not be trading every factor.\n",
    "factors_current_day = data_initial.Factor.unique()\n",
    "\n",
    "#this is just creating the portfolio from the already defined functions. \n",
    "for i in factors_current_day:\n",
    "    current_factor = data_initial[data_initial.Factor == i ] # get trades based on factor\n",
    "    initial_portfolio.append(get_valid(current_factor,maximum_net_exposure,maximum_factor_trades))  \n",
    "\n",
    "day_one = pd.concat(initial_portfolio).reset_index(drop = True) # concat single factor dfs together\n",
    "day_one['Actual Date'] = Days[0]\n",
    "day_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-process",
   "metadata": {},
   "source": [
    "***\n",
    "# Future Days\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-logging",
   "metadata": {},
   "source": [
    "## Close Trades Function \n",
    "\n",
    "This closes trades. It is simple right now but will get more complicated when prices are incorporated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dense-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#day            :is a Timestamp() object, corresponding to the date we are checking.Use Days[i] if possible. \n",
    "# prev_day_data :is a dataframe with the portfolio for the previous day. Filtered from OG dataframe. \n",
    "\n",
    "def close_trades(prev_day_data,Day):\n",
    "    prev_day_data['Exit Date'] = pd.to_datetime(prev_day_data['Exit Date'])\n",
    "    closed_trades = prev_day_data[prev_day_data['Exit Date'] == Day]\n",
    "    return closed_trades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-algebra",
   "metadata": {},
   "source": [
    "## Make Trades Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "american-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trades(prev_day_data,date,maximum_factor_trades):\n",
    "    trades = [] \n",
    "    \n",
    "    #we now want to investigate if we can make any new trades, after we have closed the other trades. \n",
    "    current_day_data = data[data['Date'] == date]\n",
    "    \n",
    "    #looping over factors, I check if we can make trades. If we can this code also randomly selects them from the potential\n",
    "    #available trades for the current day. \n",
    "    for i in current_day_data['Factor'].unique():\n",
    "        prev_day_factor = prev_day_data[prev_day_data['Factor'] == str(i)]\n",
    "        current_day_factor = current_day_data[current_day_data['Factor'] == i]\n",
    "        #this bit is a messy way of filtering out \"buy cover\" and \"sells\" <--- Change ME!!!!\n",
    "        current_day_factor_buy = current_day_factor[current_day_factor['Side'] == \"Buy\"]\n",
    "        current_day_factor_sell = current_day_factor[current_day_factor['Side'] == \"Sell Short\"]\n",
    "        current_day_factor = pd.concat([current_day_factor_buy,current_day_factor_sell])\n",
    "        \n",
    "        if len(prev_day_factor) < maximum_factor_trades:\n",
    "            if len(current_day_factor) > 0:\n",
    "                #this makes the trades, where possible. \n",
    "                trades_to_make = min(maximum_factor_trades - len(prev_day_factor),len(current_day_factor))\n",
    "                trades.append(current_day_factor.sample(n= trades_to_make))\n",
    "    if len(trades) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return pd.concat(trades)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-fishing",
   "metadata": {},
   "source": [
    "# Future Day Portfolio Construction\n",
    "\n",
    "In this function the bulk of the work is done. This is through lots of calls to the pre-defined functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mighty-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "Days = japan_2017.Date.unique()\n",
    "\n",
    "portfolio_list = []\n",
    "ct = []\n",
    "\n",
    "portfolio_list.append(day_one)\n",
    "check = []\n",
    "\n",
    "#simulates 100 days \n",
    "for i in range (1,60):\n",
    "    date = Days[i]\n",
    "    prev_day_data = portfolio_list[i-1].copy()\n",
    "    closed_trades = close_trades(prev_day_data,date)\n",
    "    ct.append(closed_trades)\n",
    "    if len(closed_trades) >0:\n",
    "        prev_day_data = pd.concat([prev_day_data, closed_trades]).drop_duplicates(keep=False)\n",
    "        \n",
    "    check.append(prev_day_data)   \n",
    "    trades = make_trades(prev_day_data,date,maximum_factor_trades)\n",
    "    \n",
    "    if len(trades) != 0:\n",
    "        portfolio_current_day = pd.concat([prev_day_data,trades])\n",
    "    else:\n",
    "        portfolio_current_day = prev_day_data \n",
    "    portfolio_current_day['Actual Date'] = np.repeat(Days[i],len(portfolio_current_day))\n",
    "    portfolio_list.append(portfolio_current_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "atlantic-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_trades = pd.concat(ct)\n",
    "closed_trades.to_csv('closed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "false-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame = pd.concat(portfolio_list)\n",
    "big_frame = big_frame.reset_index(drop = True)\n",
    "big_frame = big_frame.drop(['Unnamed: 0'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "entitled-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_frame.to_csv('open.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-spanking",
   "metadata": {},
   "source": [
    "### Getting Price Data\n",
    "\n",
    "Type portfolio_list[i] to check out a portfolio at some day in the future. We now make calls to the price data and fx files to get our proper vals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "proper-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates one big dataframe with all of our portfolios in it. This is for simplicity to extract\n",
    "#the number of shares of each company that we will buy. \n",
    "\n",
    "big_frame = pd.concat(portfolio_list)\n",
    "big_frame = big_frame.reset_index()\n",
    "big_frame = big_frame.drop(columns = ['index'])\n",
    "#properly setting up the dates of our big dataframe. \n",
    "#THIS NEEDS DONE. IT DOESNT WORK ATM!!\n",
    "\n",
    "#gets the fx data and price data from the pkl file in the correct format\n",
    "\n",
    "# fx_data2 = fx_date_conversion(fx_data)\n",
    "# price_data2 = pickle_conversion(price_data)\n",
    "\n",
    "#calls function to return dataframe of the prices and fx rate at the point at which we bought our \n",
    "#shares. \n",
    "\n",
    "#big_data = get_usd_prices(big_frame,price_data2,fx_data2)\n",
    "#big_data['Price USD per Share'] = big_data['Price']/big_data['fx_rate']\n",
    "#big_data['# Shares'] = np.floor(trade_val_init/big_data['Price USD']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "appropriate-vampire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fsym_id        Side       Date            Exit Date Country  \\\n",
      "0    NSRD0Y-R         Buy 2017-01-09  2017-01-17 00:00:00   Japan   \n",
      "1    GC5BYK-R  Sell Short 2017-01-09  2017-01-17 00:00:00   Japan   \n",
      "2    RMHHXQ-R         Buy 2017-01-09  2017-01-17 00:00:00   Japan   \n",
      "3    SL0J1F-R  Sell Short 2017-01-09  2017-01-17 00:00:00   Japan   \n",
      "4    JJ5HJX-R  Sell Short 2017-01-09  2017-01-17 00:00:00   Japan   \n",
      "..        ...         ...        ...                  ...     ...   \n",
      "243  DH0XYN-R  Sell Short 2017-01-23           2017-01-31   Japan   \n",
      "244  V3PN5M-R  Sell Short 2017-01-23           2017-01-31   Japan   \n",
      "245  N0GYLF-R  Sell Short 2017-01-23           2017-01-31   Japan   \n",
      "246  V5Z7TW-R         Buy 2017-01-23           2017-01-31   Japan   \n",
      "247  JXDLKP-R  Sell Short 2017-01-23           2017-01-31   Japan   \n",
      "\n",
      "               Sector    Factor Actual Date  \n",
      "0         All_sectors  Momentum  2017-01-09  \n",
      "1         All_sectors  Momentum  2017-01-09  \n",
      "2         All_sectors  Momentum  2017-01-09  \n",
      "3         All_sectors  Momentum  2017-01-09  \n",
      "4         All_sectors  Momentum  2017-01-09  \n",
      "..                ...       ...         ...  \n",
      "243       Industrials     Value  2017-01-23  \n",
      "244       Industrials     Value  2017-01-23  \n",
      "245  Consumer_Staples     Value  2017-01-23  \n",
      "246  Consumer_Staples     Value  2017-01-23  \n",
      "247       Industrials     Value  2017-01-23  \n",
      "\n",
      "[248 rows x 8 columns]\n",
      "0      NSRD0Y-R\n",
      "1      GC5BYK-R\n",
      "2      RMHHXQ-R\n",
      "3      SL0J1F-R\n",
      "4      JJ5HJX-R\n",
      "         ...   \n",
      "243    DH0XYN-R\n",
      "244    V3PN5M-R\n",
      "245    N0GYLF-R\n",
      "246    V5Z7TW-R\n",
      "247    JXDLKP-R\n",
      "Name: fsym_id, Length: 248, dtype: object\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['G'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3992/2338625731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mbig_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_usd_prices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_frame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprice_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mbig_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Price USD per Share'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbig_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbig_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fx_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mbig_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'# Shares'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrade_val_init\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbig_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Price USD'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3992/3482254851.py\u001b[0m in \u001b[0;36mget_usd_prices\u001b[1;34m(data, price_data, fx_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fsym_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mccy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fsym_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'currency'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mprices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprice_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mfx_data_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"USD\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mccy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fsym_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1151\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1153\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1095\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1372\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['G'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "#this code was working but has stopped - the issue is to do with the dates. I hate them!!!\n",
    "#there are so many formats and so extracting the right info I have found to be super annoying\n",
    "#as it appears sometimes pandas will autoformat a string. This is a job for tomorrow but the code\n",
    "#will work if it can correctly extract the date from price_data (which it cannot right now)\n",
    "\n",
    "\n",
    "# fx_data2 = fx_date_conversion(fx_data)\n",
    "# price_data2 = pickle_conversion(price_data)\n",
    "\n",
    "\n",
    "big_data = get_usd_prices(big_frame,price_data,fx_data)\n",
    "big_data['Price USD per Share'] = big_data['Price']/big_data['fx_rate']\n",
    "big_data['# Shares'] = np.floor(trade_val_init/big_data['Price USD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd66edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
